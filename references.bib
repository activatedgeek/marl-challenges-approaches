@misc{pommerman,
  title = {Pommerman: {AI} Research into Multi-Agent Learning},
  howpublished = {\url{https://www.pommerman.com/}},
}

@book{Sutton:1998:IRL:551283,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@inproceedings{Ng:1999:PIU:645528.657613,
 author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
 title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
 booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
 series = {ICML '99},
 year = {1999},
 isbn = {1-55860-612-2},
 pages = {278--287},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=645528.657613},
 acmid = {657613},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@book{DeJong:2002:EC:1137808,
 author = {DeJong, Kenneth A. and Jong, Kenneth A. De},
 title = {Evolutionary Computation},
 year = {2002},
 isbn = {0262041944},
 publisher = {The MIT Press},
}

@ARTICLE{2017arXiv170303864S,
   author = {{Salimans}, T. and {Ho}, J. and {Chen}, X. and {Sidor}, S. and {Sutskever}, I.},
    title = "{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1703.03864},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2017,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170303864S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@article{vinyals2017starcraft,
  title={StarCraft II: a new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@Book{Bellman:1957,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1",
  url = "http://books.google.com/books?id=fyVtp3EMxasC&pg=PR5&dq=dynamic+programming+richard+e+bellman&client=firefox-a#v=onepage&q=dynamic%20programming%20richard%20e%20bellman&f=false",
  bib2html_rescat = "General RL",
}

@book{dimitri2017dynamic,
  title={DYNAMIC PROGRAMMING AND OPTIMAL CONTROL.},
  author={Dimitri, P Bertsekas},
  year={2017},
  publisher={Athena Scientific},
}

@misc{deepmind:dccooling,
  title = {DeepMind AI Reduces Google Data Centre Cooling Bill by 40%},
  howpublished = {\url{https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/}},
}

@inproceedings{matignon2012coordinated,
  title={Coordinated Multi-Robot Exploration Under Communication Constraints Using Decentralized Markov Decision Processes.},
  author={Matignon, La{\"e}titia and Jeanpierre, Laurent and Mouaddib, Abdel-Illah and others},
  year={2012},
}

@ARTICLE{2017arXiv170203037L,
   author = {{Leibo}, J.~Z. and {Zambaldi}, V. and {Lanctot}, M. and {Marecki}, J. and 
	{Graepel}, T.},
    title = "{Multi-agent Reinforcement Learning in Sequential Social Dilemmas}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1702.03037},
 primaryClass = "cs.MA",
 keywords = {Computer Science - Multiagent Systems, Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Learning},
     year = 2017,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170203037L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@Article{Watkins1992,
author="Watkins, Christopher J. C. H. and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698",
}

@ARTICLE{2017arXiv170510257C,
   author = {{Cesa-Bianchi}, N. and {Gentile}, C. and {Lugosi}, G. and {Neu}, G.},
    title = "{Boltzmann Exploration Done Right}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1705.10257},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170510257C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group},
}

@incollection{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  booktitle={Reinforcement Learning},
  pages={5--32},
  year={1992},
  publisher={Springer},
}

@InProceedings{pmlr-v48-mniha16,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
  url = 	 {http://proceedings.mlr.press/v48/mniha16.html},
  abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
}

@ARTICLE{2017arXiv170706347S,
   author = {{Schulman}, J. and {Wolski}, F. and {Dhariwal}, P. and {Radford}, A. and 
	{Klimov}, O.},
    title = "{Proximal Policy Optimization Algorithms}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1707.06347},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2017,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170706347S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@InProceedings{pmlr-v32-silver14,
  title = 	 {Deterministic Policy Gradient Algorithms},
  author = 	 {David Silver and Guy Lever and Nicolas Heess and Thomas Degris and Daan Wierstra and Martin Riedmiller},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {387--395},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/silver14.pdf},
  url = 	 {http://proceedings.mlr.press/v32/silver14.html},
  abstract = 	 {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. Deterministic policy gradient algorithms outperformed their stochastic counterparts in several benchmark problems, particularly in high-dimensional action spaces.},
}

@ARTICLE{2015arXiv150902971L,
   author = {{Lillicrap}, T.~P. and {Hunt}, J.~J. and {Pritzel}, A. and {Heess}, N. and {Erez}, T. and {Tassa}, Y. and {Silver}, D. and {Wierstra}, D.},
    title = "{Continuous control with deep reinforcement learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1509.02971},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Statistics - Machine Learning},
     year = 2015,
    month = sep,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150902971L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@inproceedings{Littman:1994:MGF:3091574.3091594,
 author = {Littman, Michael L.},
 title = {Markov Games As a Framework for Multi-agent Reinforcement Learning},
 booktitle = {Proceedings of the Eleventh International Conference on International Conference on Machine Learning},
 series = {ICML'94},
 year = {1994},
 isbn = {1-55860-335-2},
 location = {New Brunswick, NJ, USA},
 pages = {157--163},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3091574.3091594},
 acmid = {3091594},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@article{Agogino:2012:MAM:2124496.2124503,
 author = {Agogino, Adrian K. and Tumer, Kagan},
 title = {A Multiagent Approach to Managing Air Traffic Flow},
 journal = {Autonomous Agents and Multi-Agent Systems},
 issue_date = {January   2012},
 volume = {24},
 number = {1},
 month = jan,
 year = {2012},
 issn = {1387-2532},
 pages = {1--25},
 numpages = {25},
 url = {http://dx.doi.org/10.1007/s10458-010-9142-5},
 doi = {10.1007/s10458-010-9142-5},
 acmid = {2124503},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Agent coordination, Air traffic control, Multiagent learning},
}

@INPROCEEDINGS{Pipattanasomporn:4840087, 
author={M. Pipattanasomporn and H. Feroze and S. Rahman}, 
booktitle={2009 IEEE/PES Power Systems Conference and Exposition}, 
title={Multi-agent systems in a distributed smart grid: Design and implementation}, 
year={2009}, 
volume={}, 
number={}, 
pages={1-8}, 
keywords={IP networks;distributed power generation;multi-agent systems;power engineering computing;IP-based network;agent specification;distributed smart grid;intelligent physical agent;microgrid operation;multi-agent systems;Control system synthesis;Distributed control;IP networks;Intelligent agent;Intelligent networks;Intelligent systems;Multiagent systems;Protocols;Smart grids;Technology management;Distributed smart grid;multi-agent system and microgrid}, 
doi={10.1109/PSCE.2009.4840087}, 
ISSN={}, 
month={March},
}

@article{Shoham:2007:MLA:1247754.1248180,
 author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
 title = {If Multi-agent Learning is the Answer, What is the Question?},
 journal = {Artif. Intell.},
 issue_date = {May, 2007},
 volume = {171},
 number = {7},
 month = may,
 year = {2007},
 issn = {0004-3702},
 pages = {365--377},
 numpages = {13},
 url = {https://doi.org/10.1016/j.artint.2006.02.006},
 doi = {10.1016/j.artint.2006.02.006},
 acmid = {1248180},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
}

@article{Bloembergen:2015:EDM:2831071.2831085,
 author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
 title = {Evolutionary Dynamics of Multi-agent Learning: A Survey},
 journal = {J. Artif. Int. Res.},
 issue_date = {May 2015},
 volume = {53},
 number = {1},
 month = may,
 year = {2015},
 issn = {1076-9757},
 pages = {659--697},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=2831071.2831085},
 acmid = {2831085},
 publisher = {AI Access Foundation},
 address = {USA},
}

@ARTICLE{2017arXiv170602275L,
   author = {{Lowe}, R. and {Wu}, Y. and {Tamar}, A. and {Harb}, J. and {Abbeel}, P. and 
	{Mordatch}, I.},
    title = "{Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.02275},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602275L},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@ARTICLE{Minsky:4066245, 
author={M. Minsky}, 
journal={Proceedings of the IRE}, 
title={Steps toward Artificial Intelligence}, 
year={1961}, 
volume={49}, 
number={1}, 
pages={8-30}, 
keywords={Application software;Artificial intelligence;Environmental management;High performance computing;Information processing;Libraries;Military computing;Planets;Problem-solving;Technological innovation}, 
doi={10.1109/JRPROC.1961.287775}, 
ISSN={0096-8390}, 
month={Jan},}

@phdthesis{Sutton:1984:TCA:911176,
 author = {Sutton, Richard Stuart},
 title = {Temporal Credit Assignment in Reinforcement Learning},
 year = {1984},
 note = {AAI8410337},
 publisher = {University of Massachusetts Amherst},
} 

@ARTICLE{2017arXiv170605296S,
   author = {{Sunehag}, P. and {Lever}, G. and {Gruslys}, A. and {Czarnecki}, W.~M. and 
	{Zambaldi}, V. and {Jaderberg}, M. and {Lanctot}, M. and {Sonnerat}, N. and 
	{Leibo}, J.~Z. and {Tuyls}, K. and {Graepel}, T.},
    title = "{Value-Decomposition Networks For Cooperative Multi-Agent Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.05296},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, I.2.11},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170605296S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{schmidhuber1991reinforcement,
  title={Reinforcement learning in Markovian and non-Markovian environments},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Advances in neural information processing systems},
  pages={500--506},
  year={1991}
}

@INPROCEEDINGS{Balch97learningroles,
    author = {Tucker Balch},
    title = {Learning Roles: Behavioral Diversity in Robot Teams},
    booktitle = {},
    year = {1997},
    pages = {7--12},
    publisher = {AAAI}
}

@book{Oliehoek:2016:CID:2967142,
 author = {Oliehoek, Frans A. and Amato, Christopher},
 title = {A Concise Introduction to Decentralized POMDPs},
 year = {2016},
 isbn = {3319289276, 9783319289274},
 edition = {1st},
 publisher = {Springer Publishing Company, Incorporated},
}

@article{kraemer2016multi,
  title={Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  author={Kraemer, Landon and Banerjee, Bikramjit},
  journal={Neurocomputing},
  volume={190},
  pages={82--94},
  year={2016},
  publisher={Elsevier}
}

@article{foerster2017counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1705.08926},
  year={2017}
}

@ARTICLE{2018arXiv180311485R,
   author = {{Rashid}, T. and {Samvelyan}, M. and {Schroeder de Witt}, C. and 
	{Farquhar}, G. and {Foerster}, J. and {Whiteson}, S.},
    title = "{QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1803.11485},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
     year = 2018,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180311485R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2017arXiv171003748B,
   author = {{Bansal}, T. and {Pachocki}, J. and {Sidor}, S. and {Sutskever}, I. and 
	{Mordatch}, I.},
    title = "{Emergent Complexity via Multi-Agent Competition}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1710.03748},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence},
     year = 2017,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171003748B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{2018arXiv180303835S,
   author = {{Schmitt}, S. and {Hudson}, J.~J. and {Zidek}, A. and {Osindero}, S. and 
	{Doersch}, C. and {Czarnecki}, W.~M. and {Leibo}, J.~Z. and 
	{Kuttler}, H. and {Zisserman}, A. and {Simonyan}, K. and {Eslami}, S.~M.~A.
	},
    title = "{Kickstarting Deep Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1803.03835},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2018,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303835S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}